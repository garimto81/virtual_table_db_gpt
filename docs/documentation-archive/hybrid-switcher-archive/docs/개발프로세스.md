# AI ê¸°ë°˜ ìŠ¤íŠ¸ë¦¼ë± ìŠ¤ìœ„ì¹­ ì‹œìŠ¤í…œ ê°œë°œ í”„ë¡œì„¸ìŠ¤

## ğŸš€ Phase 1: í”„ë¡œì íŠ¸ ê¸°ë°˜ êµ¬ì¶• (2ì£¼)

### 1.1 ê°œë°œ í™˜ê²½ êµ¬ì¶•
```bash
# 1ë‹¨ê³„: ì €ì¥ì†Œ ì„¤ì •
git init hybrid-switcher-ai
cd hybrid-switcher-ai

# í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„±
mkdir -p {ai-engine,companion-plugin,bridge-service,web-interface,tests,docs,deployment}

# 2ë‹¨ê³„: Python í™˜ê²½ (AI ì—”ì§„)
pyenv install 3.11.0
pyenv local 3.11.0
pip install poetry

# poetry í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
cd ai-engine
poetry init
poetry add torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
poetry add opencv-python fastapi uvicorn websockets redis
poetry add --group dev pytest black flake8 mypy

# 3ë‹¨ê³„: Node.js í™˜ê²½ (Companion í”ŒëŸ¬ê·¸ì¸)
cd ../companion-plugin
npm init -y
npm install @companion-module/base ws
npm install --save-dev typescript @types/node @types/ws
```

### 1.2 ê¸°ë³¸ ì•„í‚¤í…ì²˜ ì„¤ì •
```yaml
# docker-compose.yml
version: '3.8'
services:
  ai-engine:
    build: ./ai-engine
    ports: ["8001:8001"]
    volumes: ["./models:/app/models"]
    environment:
      - CUDA_VISIBLE_DEVICES=0
    
  bridge-service:
    build: ./bridge-service
    ports: ["8080:8080"]
    depends_on: [ai-engine, redis]
    
  redis:
    image: redis:7-alpine
    ports: ["6379:6379"]
    
  web-interface:
    build: ./web-interface
    ports: ["3000:3000"]
```

## ğŸ§  Phase 2: AI ëª¨ë¸ ê°œë°œ (4-6ì£¼)

### 2.1 ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬
```python
# Week 1-2: ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
class DataCollectionPipeline:
    def __init__(self):
        self.video_sources = []
        self.annotation_tools = AnnotationTool()
        
    def collect_poker_videos(self):
        """í¬ì»¤ ê²Œì„ ì˜ìƒ ìˆ˜ì§‘"""
        sources = [
            "twitch_poker_streams",
            "youtube_poker_content", 
            "recorded_live_games"
        ]
        return self.download_and_process(sources)
        
    def create_annotations(self, videos):
        """ìˆ˜ë™ ë¼ë²¨ë§ ë„êµ¬ë¡œ í•™ìŠµ ë°ì´í„° ìƒì„±"""
        annotations = []
        for video in videos:
            # ì¤‘ìš” ìˆœê°„ íƒ€ì„ìŠ¤íƒ¬í”„ ë§ˆí‚¹
            key_moments = self.annotation_tools.mark_key_moments(video)
            # ê²Œì„ ìƒíƒœ ë¼ë²¨ë§
            game_states = self.annotation_tools.label_game_states(video)
            annotations.append({
                'video': video,
                'key_moments': key_moments,
                'game_states': game_states
            })
        return annotations
```

### 2.2 ë¹„ì „ ëª¨ë¸ ê°œë°œ
```python
# Week 2-3: ê²Œì„ ìš”ì†Œ ê°ì§€ ëª¨ë¸
class PokerVisionModel(nn.Module):
    def __init__(self):
        super().__init__()
        # YOLOv8 ë°±ë³¸ ì‚¬ìš©
        self.backbone = YOLO('yolov8n.pt')
        self.poker_head = self.build_poker_detection_head()
        
    def build_poker_detection_head(self):
        """í¬ì»¤ íŠ¹í™” ê°ì§€ í—¤ë“œ"""
        return nn.Sequential(
            nn.Conv2d(512, 256, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(256, len(POKER_CLASSES), 1)  # ì¹´ë“œ, ì¹©, í”Œë ˆì´ì–´ ë“±
        )
        
# í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
def train_vision_model():
    model = PokerVisionModel()
    trainer = pl.Trainer(
        max_epochs=100,
        gpus=1,
        callbacks=[ModelCheckpoint(), EarlyStopping()]
    )
    trainer.fit(model, train_dataloader, val_dataloader)
```

### 2.3 ê²Œì„ ìƒí™© ì´í•´ ëª¨ë¸
```python
# Week 3-4: ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ëª¨ë¸
class GameContextTransformer(nn.Module):
    def __init__(self, d_model=512):
        super().__init__()
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model, nhead=8),
            num_layers=6
        )
        self.situation_classifier = nn.Linear(d_model, NUM_SITUATIONS)
        
    def analyze_game_sequence(self, game_events):
        """ê²Œì„ ì´ë²¤íŠ¸ ì‹œí€€ìŠ¤ì—ì„œ ìƒí™© ë¶„ì„"""
        embedded = self.embed_game_events(game_events)
        context = self.transformer(embedded)
        situation = self.situation_classifier(context[-1])
        return situation
```

### 2.4 ìŠ¤ìœ„ì¹­ ì˜ˆì¸¡ ëª¨ë¸
```python
# Week 4-5: ìŠ¤ìœ„ì¹­ íƒ€ì´ë° ì˜ˆì¸¡
class SwitchingPredictor(nn.Module):
    def __init__(self):
        super().__init__()
        self.lstm = nn.LSTM(input_size=256, hidden_size=512, num_layers=3)
        self.attention = MultiHeadAttention(512, num_heads=8)
        self.predictor = nn.Linear(512, len(SWITCHING_ACTIONS))
        
    def predict_switching_action(self, game_history, current_state):
        """ë‹¤ìŒ ìŠ¤ìœ„ì¹­ ì•¡ì…˜ê³¼ íƒ€ì´ë° ì˜ˆì¸¡"""
        lstm_out, _ = self.lstm(game_history)
        attended = self.attention(lstm_out, current_state)
        prediction = self.predictor(attended)
        return {
            'action': prediction.argmax(-1),
            'confidence': F.softmax(prediction, -1).max(),
            'timing': self.estimate_optimal_timing(prediction)
        }
```

## ğŸ”Œ Phase 3: Companion í”ŒëŸ¬ê·¸ì¸ ê°œë°œ (3-4ì£¼)

### 3.1 ê¸°ë³¸ í”ŒëŸ¬ê·¸ì¸ êµ¬ì¡°
```typescript
// Week 6: Companion í”ŒëŸ¬ê·¸ì¸ ê¸°ë³¸ êµ¬ì¡°
// companion-plugin/src/index.ts
import { InstanceBase, InstanceStatus } from '@companion-module/base'

export class HybridSwitcherInstance extends InstanceBase<Config> {
    private aiConnection: WebSocket | null = null
    private currentSuggestion: AISuggestion | null = null
    
    async init(config: Config): Promise<void> {
        this.config = config
        this.updateStatus(InstanceStatus.Connecting)
        
        await this.connectToAIService()
        this.initActions()
        this.initFeedbacks()
        this.initPresets()
        
        this.updateStatus(InstanceStatus.Ok)
    }
    
    private async connectToAIService(): Promise<void> {
        const wsUrl = `ws://${this.config.host}:${this.config.port}/ws`
        this.aiConnection = new WebSocket(wsUrl)
        
        this.aiConnection.onmessage = (event) => {
            const message = JSON.parse(event.data)
            this.handleAIMessage(message)
        }
    }
}
```

### 3.2 ë™ì  UI ê°œë°œ
```typescript
// Week 7: ë²„íŠ¼ ìƒíƒœ ê´€ë¦¬ ì‹œìŠ¤í…œ
class ButtonManager {
    updateSuggestionButtons(suggestion: AISuggestion): void {
        // AI ì‹ ë¢°ë„ì— ë”°ë¥¸ ë™ì  ìƒ‰ìƒ
        const confidenceColor = this.getConfidenceColor(suggestion.confidence)
        
        this.setButtonState('accept_suggestion', {
            text: `ìˆ˜ë½ ${Math.round(suggestion.confidence * 100)}%`,
            bgcolor: confidenceColor,
            blink: suggestion.confidence > 0.9
        })
        
        // ì¹´ìš´íŠ¸ë‹¤ìš´ íƒ€ì´ë¨¸ ì‹œì‘
        if (suggestion.auto_execute) {
            this.startCountdown('accept_suggestion', suggestion.timeout)
        }
    }
    
    private startCountdown(buttonId: string, seconds: number): void {
        let remaining = seconds
        const interval = setInterval(() => {
            if (remaining <= 0) {
                clearInterval(interval)
                this.executeAutoAction(buttonId)
                return
            }
            this.updateButtonText(buttonId, `ìˆ˜ë½ (${remaining}s)`)
            remaining--
        }, 1000)
    }
}
```

### 3.3 í”ŒëŸ¬ê·¸ì¸ íŒ¨í‚¤ì§•
```json
// Week 8: ë°°í¬ ì¤€ë¹„
{
  "name": "@hybrid-switcher/companion-plugin",
  "version": "1.0.0",
  "main": "dist/index.js",
  "scripts": {
    "build": "tsc",
    "package": "npm pack",
    "install-companion": "cp *.tgz ~/.companion/plugins/"
  },
  "companion": {
    "minimumVersion": "3.0.0",
    "category": "Broadcasting"
  }
}
```

## ğŸŒ‰ Phase 4: í†µí•© ì‹œìŠ¤í…œ ê°œë°œ (3-4ì£¼)

### 4.1 ë¸Œë¦¬ì§€ ì„œë¹„ìŠ¤ ê°œë°œ
```python
# Week 9: AI ì—”ì§„ê³¼ Companion ê°„ ë¸Œë¦¬ì§€
from fastapi import FastAPI, WebSocket
import asyncio
import json

app = FastAPI()

class BridgeService:
    def __init__(self):
        self.ai_client = AIEngineClient()
        self.companion_connections = set()
        
    async def handle_companion_connection(self, websocket: WebSocket):
        await websocket.accept()
        self.companion_connections.add(websocket)
        
        try:
            while True:
                # Companionì—ì„œ ë©”ì‹œì§€ ìˆ˜ì‹ 
                message = await websocket.receive_text()
                await self.handle_companion_message(json.loads(message))
        except:
            self.companion_connections.remove(websocket)
            
    async def handle_ai_suggestion(self, suggestion):
        """AI ì œì•ˆì„ ëª¨ë“  ì—°ê²°ëœ Companionìœ¼ë¡œ ì „ì†¡"""
        message = json.dumps({
            'type': 'ai_suggestion',
            'data': suggestion
        })
        
        for connection in self.companion_connections:
            await connection.send_text(message)

@app.websocket("/ws/companion")
async def companion_websocket(websocket: WebSocket):
    bridge = BridgeService()
    await bridge.handle_companion_connection(websocket)
```

### 4.2 ì‹¤ì‹œê°„ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
```python
# Week 10: ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ì²˜ë¦¬
import asyncio
import cv2
from concurrent.futures import ThreadPoolExecutor

class RealTimeProcessor:
    def __init__(self):
        self.ai_model = load_trained_model()
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.frame_queue = asyncio.Queue(maxsize=30)
        
    async def process_video_stream(self, video_source):
        """ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬"""
        cap = cv2.VideoCapture(video_source)
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
                
            # ë¹„ë™ê¸° AI ì¶”ë¡ 
            loop = asyncio.get_event_loop()
            future = loop.run_in_executor(
                self.executor, 
                self.ai_model.inference, 
                frame
            )
            
            # ê²°ê³¼ë¥¼ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  ë‹¤ìŒ í”„ë ˆì„ ì²˜ë¦¬
            asyncio.create_task(self.handle_inference_result(future))
            
            await asyncio.sleep(1/30)  # 30 FPS
            
    async def handle_inference_result(self, future):
        result = await future
        if result['confidence'] > 0.7:
            await self.send_suggestion_to_companion(result)
```

## ğŸ§ª Phase 5: í…ŒìŠ¤íŠ¸ ë° ìµœì í™” (2-3ì£¼)

### 5.1 ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
```python
# Week 11: AI ëª¨ë¸ í…ŒìŠ¤íŠ¸
import pytest
import torch

class TestAIModels:
    def test_vision_model_inference(self):
        model = PokerVisionModel()
        dummy_frame = torch.randn(1, 3, 720, 1280)
        
        with torch.no_grad():
            result = model(dummy_frame)
            
        assert result['detections'].shape[0] > 0
        assert 0 <= result['confidence'].item() <= 1
        
    def test_prediction_latency(self):
        model = PokerVisionModel().cuda()
        dummy_input = torch.randn(1, 3, 720, 1280).cuda()
        
        import time
        start_time = time.time()
        
        for _ in range(100):
            _ = model(dummy_input)
            torch.cuda.synchronize()
            
        avg_time = (time.time() - start_time) / 100
        assert avg_time < 0.05  # 50ms ì´í•˜
```

### 5.2 í†µí•© í…ŒìŠ¤íŠ¸
```typescript
// Companion í”ŒëŸ¬ê·¸ì¸ í…ŒìŠ¤íŠ¸
describe('Companion Integration', () => {
    let mockAIService: MockAIService
    let instance: HybridSwitcherInstance
    
    beforeEach(async () => {
        mockAIService = new MockAIService()
        await mockAIService.start()
        
        instance = new HybridSwitcherInstance()
        await instance.init({
            host: 'localhost',
            port: mockAIService.port
        })
    })
    
    test('should handle AI suggestions', async () => {
        const suggestion = {
            action: 'camera_switch',
            confidence: 0.85,
            timeout: 10
        }
        
        mockAIService.sendSuggestion(suggestion)
        await new Promise(resolve => setTimeout(resolve, 100))
        
        expect(instance.getCurrentSuggestion()).toEqual(suggestion)
    })
})
```

### 5.3 ì„±ëŠ¥ ìµœì í™”
```python
# Week 12: ì„±ëŠ¥ ìµœì í™”
class OptimizedInference:
    def __init__(self, model_path):
        # TensorRT ìµœì í™”
        self.model = torch.jit.load(model_path)
        self.model = torch_tensorrt.compile(self.model)
        
        # ë©”ëª¨ë¦¬ í’€ë§
        self.memory_pool = MemoryPool()
        
    @torch.no_grad()
    async def optimized_inference(self, frame):
        # ë°˜ì •ë°€ë„ ì—°ì‚°
        with torch.cuda.amp.autocast():
            result = self.model(frame.half())
            
        return result.float()
```

## ğŸš€ Phase 6: ë°°í¬ ë° ìš´ì˜ (1-2ì£¼)

### 6.1 Docker ì»¨í…Œì´ë„ˆí™”
```dockerfile
# ai-engine/Dockerfile
FROM nvidia/cuda:11.8-runtime-ubuntu20.04

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8001

CMD ["python", "-m", "ai_engine.main"]
```

### 6.2 CI/CD íŒŒì´í”„ë¼ì¸
```yaml
# .github/workflows/deploy.yml
name: Deploy

on:
  push:
    branches: [main]

jobs:
  test-and-deploy:
    runs-on: ubuntu-latest-gpu
    steps:
      - uses: actions/checkout@v3
      
      - name: Run tests
        run: |
          python -m pytest ai-engine/tests/
          npm test --prefix companion-plugin
          
      - name: Build containers
        run: docker-compose build
        
      - name: Deploy to staging
        run: |
          docker-compose -f docker-compose.staging.yml up -d
          
      - name: Run integration tests
        run: python integration-tests/test_full_system.py
        
      - name: Deploy to production
        if: success()
        run: |
          docker-compose -f docker-compose.prod.yml up -d
```

## ğŸ“Š ê°œë°œ ì¼ì • ìš”ì•½

| Phase | ê¸°ê°„ | ì£¼ìš” ì‚°ì¶œë¬¼ | ë¦¬ì†ŒìŠ¤ |
|-------|------|-------------|---------|
| Phase 1 | 2ì£¼ | ê°œë°œí™˜ê²½, ê¸°ë³¸ êµ¬ì¡° | 1ëª… (DevOps) |
| Phase 2 | 4-6ì£¼ | AI ëª¨ë¸, í•™ìŠµ íŒŒì´í”„ë¼ì¸ | 2ëª… (AI ì—”ì§€ë‹ˆì–´) |
| Phase 3 | 3-4ì£¼ | Companion í”ŒëŸ¬ê·¸ì¸ | 1ëª… (Frontend) |
| Phase 4 | 3-4ì£¼ | í†µí•© ì‹œìŠ¤í…œ, ë¸Œë¦¬ì§€ | 1ëª… (Backend) |
| Phase 5 | 2-3ì£¼ | í…ŒìŠ¤íŠ¸, ìµœì í™” | ì „ì²´ íŒ€ |
| Phase 6 | 1-2ì£¼ | ë°°í¬, ìš´ì˜ | 1ëª… (DevOps) |

**ì´ ê°œë°œ ê¸°ê°„**: 15-21ì£¼ (ì•½ 4-5ê°œì›”)
**í•„ìš” ì¸ë ¥**: 5ëª… (AI ì—”ì§€ë‹ˆì–´ 2ëª…, Backend 1ëª…, Frontend 1ëª…, DevOps 1ëª…)

## ğŸ¯ ì„±ê³µ ê¸°ì¤€

### ê¸°ìˆ ì  ëª©í‘œ
- AI ì˜ˆì¸¡ ì •í™•ë„: 85% ì´ìƒ
- ì‹œìŠ¤í…œ ì‘ë‹µ ì‹œê°„: 50ms ì´í•˜
- ê°€ë™ë¥ : 99.9% ì´ìƒ

### ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œ
- ìš´ì˜ì í”¼ë¡œë„: 70% ê°ì†Œ
- ë°©ì†¡ í’ˆì§ˆ í–¥ìƒ: ì¸¡ì • ê°€ëŠ¥í•œ ì§€í‘œ
- ì‚¬ìš©ì ë§Œì¡±ë„: 4.5/5.0 ì´ìƒ

ì´ ê°œë°œ í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ ì™„ì „í•œ AI ê¸°ë°˜ ìŠ¤íŠ¸ë¦¼ë± ìŠ¤ìœ„ì¹­ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!